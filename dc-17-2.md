## Wednesday

@golubbe

## Head of infrastructure and operations at Visa

@ksr_swamy

Speed and efficiency
Provisioning time: weeks, days
Patching: difficult

Infrastructure growing much faster than headcounts

Machines would decomm in ~3 months

Lessons learned:
* Granularity: big images problematic
* memory footprint
* load balancing - wish there were more features

Visa is about to expand to 5 groups (in other words, they are still in closed beta)

Back to the original dude.  Emphasize polyglot infrastructure.

"start with a secure base and containerize apps"

"build a supply chain" - container as a service

Securing the process from dev to build

Tag containers with things like 'PII' - which would mean "can't deploy to public cloud".  Enable devs to define their security policy needs.

Docker trusted registry has "policies" configurable

Announcements for vendorexpansions: linux for z Systems and power series (something about mainframes)
Docker on GoogleComputePlatform and AlibabaCloud

image2docker converts vmdk to a container

Containerize an oracle DB.  https://store.docker.com/editions/enterprise/docker-ee-server-oraclelinux?tab=description
BOOM

Mark Cavage VP product dev at Oracle

Docker Modernize Traditional Apps program: "turnkey" upgrades to Docker Enterprise without changing source
http://docker.com/MTA

## Aaron Ades AVP solutions engineer MetLife

Talks about th ehistory, 1868 to present (1950s univac, 1982: still have code from 1982 running in production, 2000 goes public)

Talks about the strategy to decompose and retire legacy apps:
* Wrap (wrap the application logic)
* Tap (uhh something about different backends)
* Scrap

Open benefits enrollment: huge traffic surge (25x).  Utilize public cloud (Azure) for surge workloads

"In some extreme cases, almost 70% consolidation" - higher utilization

Story: they formed a team called "MOD squad" in which they made small cross-functional teams.  He's all excited about the chatops angle.  "Most fulfilling project ever worked on".  Did it in 5 months - fuck me

## burnout

Maslach Burnout Inventory
http://www.mindgarden.com/117-maslach-burnout-inventory

* Exhaustion
* Cynicism
* Efficacy

## Tim Tyler Solutions Engineer, MetLife

Customer-facing applications

The idea of "enterprise journey" is a big theme

Started with "let's build a cluster".  Tossed out 'waterfall'.

Built a team:
* cross functional
* very focused
* high diversity
* freedom to explore
* Built a new area for the team
* Solved problems on-demand as opposed to making meetings
* Every member empowered to make decisions within group context

Brought in open source tech, which they basically did not do.
* docker data center 1.12
* redis
* grafana
* prometheus
* rabbit

Still battling culture.
Do they need an Open Source Governance Model?

Process and procedure: still not figured out.  For example: who owns the label dictionary?
Takeaway: "mod squad" team should have done a better job engaging application teams broadly to surface governance questions.

Tips:
* tag and label cluser engines and nodes
* label early and label often
  * geo code
  * test script
  * environment
  * maturity
  * maintainer
  * ops guide location
* follow a label convention `com.company.docker.something.helpful`
* Lint your YAML and compose files
* Chaos monkey: built a doc with failure modes
* Held War Games with the ops team:
  * Done over a week
  * Using prometheus
  * Did not sit with ops while app devs broke the shit
* Going to try https://github.com/sstephenson/bats :
* Have a stable demo environment:
* "break" Demo and let it heal (prometheus and grafana)
* histrix dashboard: https://github.com/Netflix/Hystrix/wiki/Dashboard

Training:
* Expect resistance
* Plan shallow and deep dives
* Plan to do it again and again

Operational handoff
* Don't underestimate it
* "Everything is the same" but it's all brand new

Started in cloud, then in-house.  This is more like athena than other use cases.

Persistent storage _is the elephant in the room_ - is there a right way?  They went super simple.  (elephants and whales are both in order _Artiodactyla_, Undulates)

They have databases in containers even, API gateway in a container

* team of equals that thrived
* seeds of DevOps
* renewed open source interest
* ChatOps

Getting there fast caused a lot of disruption

## Become troubleshooting pro
Jeff Anderson
@programm3rq

### Put data in a volume

`docker diff <image>` - diff filesystem from image
`docker cp` - copy files from/into volume

### Mount local

`sqlite3 -bail test.db` - fails because file permissions

Takeaway: the numeric uid is what matters
Files created by container owned by uid = 0

Prefer `chown $( id -u ) <file>`

Docker for Mac actually does magic to ensure files written by containers match your mac uid

### Networking

Suppose application together with nginx file
Unexpected 502 bad gateway!

The reason: curl from nginx (over localhost) to app fails

localhost, eth0 of docker host.
Each container gets a full copy of the full networking namespace

How to get rid of hardcoded IP and app port publish?

Have to add docker network service discovery:
* Docker runs a resolver at 127.0.0.11
* Resolves container IPs by name or alias
* nginx config: `proxy_pass htttp://web:8000` and `resolver: 127.0.0.11`

### universal control plane
UCP exposes docker daemon API on port 443
The web app provides the docker daemon API through the same interface as the GUI

Client bundle: causes issue with docker-compose but not docker CLI

Useful helpful thing!  A command for examining certs
`openssl x509 -noout -text < example.pem`

Toolbox!
`socat` - bidirectional TCP/DUP/stdio/pipes/sockets
`jq` - CLI for JSON
`nsenter` - enter arbitrary Linux namespace

```
socat -v tcp4-listen:5566,bind=127.0.0.1,reuseaddr,fork unix-connect:/var/run/docker.sock`
docker -H localhost:5566 run 
```

```
curl -s --unix-socket /var/run/docker.sock http://containers/json
```

Bunch of namespaces: PID namespace, ingress host namespace

Suppose host A is working and host B is not doing networking well.

How to Ask a Question

```
<statement of observation>

|---------------------------------------|
| demonstration of relevant observations|
|---------------------------------------|

<question>
```

Forums:
https://forums.docker.com
Slack:
https://dockr.ly/community


## Docker Networking in Production at Visa

@churchofmark Mark Church from Docker
Sasi Kannappan from Visa

Talk about the Visa story and unanticipated problems

Speak in a different language for networking
* 100s or more containers per host
* container lasts minutes or months
* more horizontal distribution of netork traffic (E-W)

Container Network Model is the "contract" or interface between the engine and the networking layer
* Portable policies
* Plugin design (batteries included but removable)

Visa
In production for 6 months
* ~100 containers running atm, can scale to ~800
* 2 prod, 2 sandbox, 2 regions

Core systems in a private cloud, based on virtualized models

Looking at huge transactional growth year-over-year.  Needed to be able to scale continuously instead of periodic major refactors.

Goals:
* microservice architecture
* dynamic scalability
* operational simplicity
* load balancer-less

First pass:
* scheduling - ucp
* service registration - gliderlabs + consul
* service discovery - consul + dns
* load balancing - consul round robin DNS & health check
* connectivity - docker bridge driver (as opposed to overlay)

Each host has consul agent, registrator: extra complexity.  Extra health checks on those additional components

Maybe the takeaway is that rolling your own scheduler is trouble.

Then Docker made Docker Networking Architecture: this colocates a DNS service with each docker container

Every service gets a VIP and a DNS entry

Docker 1.12 introduced health check and built-in load balancing https://blog.docker.com/2016/06/docker-1-12-built-in-orchestration/ with Swarm

Second pass:
* scheduling - ucp
* service registration - docker engine
* service discovery - docker engine dns
* load balancing - swarm VIP LB
* connectivity - docker overlay driver

Bridge: choosing ports runs out of nodes
Overlay simplifies much; configure your app to have some internal port mapped to a docker-managed overlay network port

https://success.docker.com

## Closing keynote

Cool Hacks

### play-with-docker
Mantika - http://labs.play-with-docker.com

Can share state by passing around URLs

Can create multi-node swarms

Can use play-with-docker as a docker-machine driver

`PWD_URL=whatever docker-machine create -d pwd`

How does it work?

ELB -> sends to a swarm host.  Each has a PWD daemon.
Created session creates an overlay network
Created instance is an isolated Docker in Docker

3500 containers per host (r3.4xlarge)
Swarm in Swarm

In Docker, they are using internally for training.

### Alex Ellis 
@alexellisuk

app dev ADP

Serverless - Function as a Service

https://github.com/alexellis/faas

Amazon uses for echo

minio-DB


## Ben Golub
@golubbe

# Thursday

## But I'm a SysAdmin

Mike Coleman @mikegcoleman tech evangelist at Docker

IT professional more than developer

VM and containers have fundamentally different benefits.
A VM is a house.  Can only get as small as MBs, generally GBs
A container is an apartment.  Docker engine is the doorman.  Containers _share_ the kernel.

Why docker on bare metal?  Typically, performance (skip the hypervisor)
But problem: give up on the benefits of consolidation.

Most people are doing docker on VMs, though, because high performance less important.

### Container consolidation testing
* docker cs 1.12.3, vmware, commodity hardware
with sysbench to test capacity

8 VMs w/ 1 workload compared to 1 VM w/ 8 containers: increase utilization by 25%, 30% less storage, 7% less RAM

8VMs 1 workload each compared to 8 containers bare metal: increase utilization by 47%

They needed to tune: CPU pinning etc.  Without tuning, performance was _worse_

### Security

Security is not just isolation.
* patches
* image signing/fidelity
* sensitive data/passwords (evidently environment variables considered wildly insecure)

Key features (plug for docker datacenter or CE swarm):
* usable security
* trusted delivery
* infrastructure independent

Trusted Registry allows for multiple signatures on a given image

### Docker for AWS

Docker CE for AWS on docker store starts of with a cloud formation template

"Use cloudwatch for container logging?"

Does a TON of work for you.  Can SSH into managers but workers are all closed.

ssh in, `docker node ls` `docker service ls`

Half of people are moving apps from VMs to containers.

### Big takeaways
* Need to involve everyone together:
  * dev
  * ops
  * security
  * executive
  * performance
* Choose good partners
* Define guardrails: handle one application with defined success criteria
* What's the plan for:
  * port mappings
  * environment configs
  * persistent data

## What have Namespaces done for you lately

Liz Rice, @lizrice from Aqua Security

* namespaces
* cgroups
* fork bomb

### namespaces
Syscall interface
* timeshare system
* proc ids
* mounts (original namespace, so the flag is NS)
* network
* uids
* IPC

### demo
Demo written in golang

```
func run() {
  //cmd := exec.Command(os.Args[2], os.Args[3:]...)
  cmd = exec.Command("/proc/self/exe", "child". args...);
  cmd.Stdin = os.Stdin
  cmd.Stdout = os.Stdout
  cmd.Stderr = os.Stderr
  cmd.SysProcAttr = syscall.SysProcAttr{
    Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS,
  }
  cmd.Run()
}
func child() {
  cmd := exec.Command(os.Args[2], os.Args[3:]...)
  cmd.Stdin = os.Stdin
  cmd.Stdout = os.Stdout
  cmd.Stderr = os.Stderr
  syscall.Sethostname('container')
  syscall.Chroot('/path/to/copy')
  syscall.Mount("proc", "proc", "proc", 0, "")
  syscall.Mount("something", "my_temp", "tmpfs", 0, "")  // written to memory rather than disk
}
```

Steps:
* Need a `run` and a `child`: mods to env in the child
* chroot to get isolated env
* need to mount proc pseudo directory
* need to create a "mount namespace" (NS) so that mounts are isolated from host

Super important fact
* Stuff in container is visible in host `/proc`.  _ENVIRONMENT VARIABLES ARE NOT SECRET IN DOCKER RUNTIME_.  You can also kill proc from host, etc.


### cgroups
/sys/fs/cgroup/memory/docker

Contains files whose values are the cgroup configuration

pids.max => 20
notify_on_release => 1

`:(){ :|: & };:` - create a function named `:`, which calls itself and pipes to itself and forks off

Used as a demo to show that cgroup process limits she implemented worked

## Container performance analysis

Brendan Gregg performance architect Netflix

* identify issues in host v container using system metrics
* app code w/ flame graphs
* kernel tracing tools

Netflix : titus project for cloud runtime platform
* scheduling
* container execution
* http://techblog.netflix.com/2017/04/the-evolution-of-container-usage-at.html
* over 1M containers deployed a week

stream processing: flink
UI: nodejs

cgroups (control groups) restrict usage
cgroup + namespace = container

### CPU shares
CPU limit = 100% * container's shares/total busy shares
Minimum guarantee = 100% * container's shares/total allocated shares

Can make performance anaylsis complicated, since resource allocation is dynamic

### OS config
FS:
https://docs.docker.com/engine/userguide/storagedriver/

Networking

### USE method
Draw a diagram of the system, ID each resource, for each check
* utilization (eg time busy)
* saturation (eg run queue length)
* errors (eg ECC errors)

### Host tools
...assuming you have host access

"most of the time, it's not the container that's to blame" - often just the physical resources

https://brendangregg.com/linuxperf.html
http://techblog.netflix.com/2015/11/linux-performance-analysis-in-60s.html
use method linux

Event tracing: iosnoop (from perf-tools, or BCC project)

systemd-cgtop - a "top" for cgroups

`docker stats` block i/o, PIDS, utilization

`/proc/*/ns/*`

`nsenter -t <PID> -u hostname`
`nsenter -t <PID> -m -p top`

Profiling
naive:
`perf record -F 49 -a -g -- sleep 30; perf script`
Doesn't work: perf not updated

https://github.com/brendangregg/FlameGraph

/sys/fs/cgroup/.../cpu.stat
/proc/PID/status nonvoluntary_ctxt_switches

Hard to diagnose within a container: mixed up host and resident stats

### Tracing

BPF - kernel virtual machine
https://github.com/iovisor/bcc

Slides: https://www.slideshare.net/brendangregg/container-performance-analysis
